{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ Classificador de Frutas com Rede Neural Convolucional (CNN)\n",
    "\n",
    "Este notebook apresenta um sistema completo de **classificaÃ§Ã£o de frutas** utilizando **Deep Learning**, baseado em uma **Rede Neural Convolucional (CNN)** desenvolvida com **TensorFlow/Keras**.\n",
    "\n",
    "O objetivo Ã© treinar um modelo capaz de identificar diferentes tipos de frutas a partir de imagens, utilizando um pipeline robusto de preparaÃ§Ã£o dos dados, construÃ§Ã£o do modelo, monitoramento de mÃ©tricas e salvamento de resultados.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Œ Funcionalidades do Notebook\n",
    "\n",
    "### **1. PrÃ©-processamento das imagens**\n",
    "- Redimensionamento automÃ¡tico para o tamanho definido\n",
    "- NormalizaÃ§Ã£o dos pixels para acelerar o treinamento\n",
    "- *Data augmentation* para reduzir overfitting e aumentar a robustez:\n",
    "  - rotaÃ§Ãµes\n",
    "  - zoom\n",
    "  - flips horizontais\n",
    "  - variaÃ§Ãµes de brilho\n",
    "\n",
    "---\n",
    "\n",
    "### **2. ConstruÃ§Ã£o do Modelo (CNN)**\n",
    "A arquitetura inclui:\n",
    "- MÃºltiplas camadas **Convolution2D**\n",
    "- **MaxPooling2D** para reduÃ§Ã£o de dimensionalidade\n",
    "- **Batch Normalization** para estabilizar o treino\n",
    "- **Dropout** para diminuir overfitting\n",
    "- Otimizador **Adam**\n",
    "- FunÃ§Ã£o de perda **categorical_crossentropy**\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Treinamento e ValidaÃ§Ã£o**\n",
    "- SeparaÃ§Ã£o dos dados em **treino**, **validaÃ§Ã£o** e **teste**\n",
    "- Callbacks profissionais:\n",
    "  - **EarlyStopping** (interrompe quando nÃ£o melhora)\n",
    "  - **ModelCheckpoint** (salva o melhor modelo)\n",
    "- MÃ©tricas acompanhadas:\n",
    "  - `accuracy`, `val_accuracy`\n",
    "  - `loss`, `val_loss`\n",
    "- Pipeline de dados otimizado com `tf.data.AUTOTUNE` para alto desempenho\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Monitoramento com MLflow**\n",
    "O MLflow registra automaticamente:\n",
    "- ParÃ¢metros do experimento\n",
    "- HiperparÃ¢metros do modelo\n",
    "- Curvas de treino e validaÃ§Ã£o\n",
    "- Arquitetura e pesos do modelo (artefatos)\n",
    "- Melhor versÃ£o do modelo salvo\n",
    "\n",
    "Isso permite comparar execuÃ§Ãµes e reproduzir treinamentos.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. PrediÃ§Ã£o em Novas Imagens**\n",
    "- Carregamento e prÃ©-processamento automÃ¡tico\n",
    "- InferÃªncia pelo modelo treinado\n",
    "- ExibiÃ§Ã£o da imagem com a **classe prevista**\n",
    "- Probabilidade da previsÃ£o exibida junto ao resultado\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ› ï¸ Tecnologias Utilizadas\n",
    "- **Python**\n",
    "- **TensorFlow / Keras**\n",
    "- **NumPy**\n",
    "- **OpenCV**\n",
    "- **Matplotlib**\n",
    "- **MLflow**\n",
    "- **GPU com Memory Growth ativado** para evitar problemas de VRAM\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download AutomÃ¡tico do Dataset (Kaggle)\n",
    "\n",
    "Nesta etapa, o notebook realiza o **download automÃ¡tico do dataset** diretamente do Kaggle usando a biblioteca `kagglehub`.  \n",
    "Para isso, Ã© criado o arquivo `kaggle.json` contendo as credenciais da conta, permitindo o acesso Ã  API do Kaggle.\n",
    "\n",
    "**O que o cÃ³digo faz:**\n",
    "1. Instala a biblioteca `kagglehub` (caso nÃ£o esteja instalada).\n",
    "2. Cria a pasta `~/.kaggle` caso ela ainda nÃ£o exista.\n",
    "3. Gera o arquivo `kaggle.json` com seu nome de usuÃ¡rio e chave de API.\n",
    "4. Ajusta permissÃµes do arquivo para que o Kaggle aceite o acesso.\n",
    "5. Baixa automaticamente o dataset **\"Fruit Classification (10 classes)\"**.\n",
    "6. Exibe o caminho da pasta onde o dataset foi salvo.\n",
    "\n",
    "ApÃ³s esta etapa, o dataset estÃ¡ disponÃ­vel localmente e pode ser carregado para prÃ©-processamento e treinamento do modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub --quiet\n",
    "import os, json\n",
    "\n",
    "kaggle_config = {\n",
    "    \"username\": \"SEU_USUARIO\",\n",
    "    \"key\": \"SUA_SENHA\"}\n",
    "\n",
    "os.makedirs(os.path.expanduser(\"~/.kaggle\"), exist_ok=True)\n",
    "with open(os.path.expanduser(\"~/.kaggle/kaggle.json\"), \"w\") as f:\n",
    "    json.dump(kaggle_config, f)\n",
    "\n",
    "os.chmod(os.path.expanduser(\"~/.kaggle/kaggle.json\"), 0o600)\n",
    "\n",
    "import kagglehub\n",
    "\n",
    "dataset_handle = \"karimabdulnabi/fruit-classification10-class\"\n",
    "path = kagglehub.dataset_download(dataset_handle, force_download=True)\n",
    "\n",
    "print(\"âœ… Dataset baixado em:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PadronizaÃ§Ã£o dos nomes das classes\n",
    "\n",
    "Como o dataset pode conter pastas com nomes inconsistentes (ex.: `stawberries`, `pinenapple`), esta etapa garante que **todas as classes tenham nomes padronizados e corrigidos** antes do treinamento.\n",
    "\n",
    "O cÃ³digo faz:\n",
    "\n",
    "1. **LÃª automaticamente** os nomes das pastas dentro dos diretÃ³rios `train` e `test`;\n",
    "2. Aplica um dicionÃ¡rio de correÃ§Ãµes (`standard_names`), corrigindo erros de digitaÃ§Ã£o e padronizando capitalizaÃ§Ã£o;\n",
    "3. Retorna a lista final ordenada das classes jÃ¡ corrigidas.\n",
    "\n",
    "Isso evita que classes diferentes sejam tratadas como distintas apenas devido a erros de escrita no nome da pasta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# DicionÃ¡rio de padronizaÃ§Ã£o\n",
    "standard_names = {\n",
    "    'apple': 'Apple',\n",
    "    'banana': 'Banana',\n",
    "    'avocado': 'Avocado',\n",
    "    'cherry': 'Cherry',\n",
    "    'kiwi': 'Kiwi',\n",
    "    'mango': 'Mango',\n",
    "    'orange': 'Orange',\n",
    "    'pinenapple': 'Pineapple',\n",
    "    'pineapple': 'Pineapple',\n",
    "    'strawberries': 'Strawberries',\n",
    "    'stawberries': 'Strawberries',\n",
    "    'watermelon': 'Watermelon'\n",
    "}\n",
    "\n",
    "def standardize_class_names(class_list):\n",
    "    return sorted([standard_names.get(c.lower(), c) for c in class_list])\n",
    "\n",
    "# âš™ï¸ LÃª as classes diretamente das pastas\n",
    "train_dir = '/home/wallingson12/Github/Data science and ML/Classificador de frutas/train'\n",
    "validation_dir = '/home/wallingson12/Github/Data science and ML/Classificador de frutas/test'\n",
    "\n",
    "train_classes_raw = [d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))]\n",
    "val_classes_raw = [d for d in os.listdir(validation_dir) if os.path.isdir(os.path.join(validation_dir, d))]\n",
    "\n",
    "# âœ… Padroniza\n",
    "train_std = standardize_class_names(train_classes_raw)\n",
    "val_std   = standardize_class_names(val_classes_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VerificaÃ§Ã£o das classes apÃ³s padronizaÃ§Ã£o\n",
    "\n",
    "ApÃ³s corrigir e padronizar os nomes das pastas, Ã© importante confirmar se o conjunto de **treino** e o de **validaÃ§Ã£o** possuem exatamente as mesmas classes.\n",
    "\n",
    "O cÃ³digo:\n",
    "\n",
    "- Exibe no console as classes padronizadas de cada conjunto;\n",
    "- Compara ambas as listas para garantir que nÃ£o existe uma fruta presente somente em um dos conjuntos;\n",
    "- Caso haja discrepÃ¢ncias, ele indica quais classes estÃ£o faltando em cada conjunto.\n",
    "\n",
    "Isso evita erros futuros, como a rede ser treinada com uma classe que nÃ£o aparece no conjunto de validaÃ§Ã£o, o que afetaria as mÃ©tricas de avaliaÃ§Ã£o e o processo de generalizaÃ§Ã£o."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"â–¶ Classes padronizadas (treino):\", train_std)\n",
    "print(\"â–¶ Classes padronizadas (validaÃ§Ã£o):\", val_std)\n",
    "\n",
    "# Verifica se hÃ¡ classes faltando em algum conjunto\n",
    "missing_in_val = set(train_std) - set(val_std)\n",
    "missing_in_train = set(val_std) - set(train_std)\n",
    "\n",
    "if not missing_in_val and not missing_in_train:\n",
    "    print(\"\\nPerfeito! Treino e validaÃ§Ã£o agora possuem as MESMAS classes.\")\n",
    "else:\n",
    "    print(\"\\nâš  DiferenÃ§as encontradas:\")\n",
    "    if missing_in_val:\n",
    "        print(\" - Presentes no treino mas ausentes na validaÃ§Ã£o:\", missing_in_val)\n",
    "    if missing_in_train:\n",
    "        print(\" - Presentes na validaÃ§Ã£o mas ausentes no treino:\", missing_in_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation e PrÃ©-processamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "import tensorflow as tf\n",
    "\n",
    "def build_model(image_size, num_classes):\n",
    "    l2 = regularizers.l2(1e-4)\n",
    "\n",
    "    data_augmentation = tf.keras.Sequential([\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.05),\n",
    "        layers.RandomZoom(0.10),\n",
    "        layers.RandomContrast(0.10),\n",
    "    ])\n",
    "\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(image_size[0], image_size[1], 3)),\n",
    "        data_augmentation,\n",
    "        layers.Rescaling(1./255),\n",
    "\n",
    "        # BLOCO 1\n",
    "        layers.Conv2D(64, 3, padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(64, 3, padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.15),\n",
    "\n",
    "        # BLOCO 2\n",
    "        layers.Conv2D(128, 3, padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(128, 3, padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.20),\n",
    "\n",
    "        # BLOCO 3\n",
    "        layers.Conv2D(256, 3, padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Conv2D(256, 3, padding='same', kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.MaxPooling2D(),\n",
    "        layers.Dropout(0.25),\n",
    "\n",
    "        layers.GlobalAveragePooling2D(),\n",
    "\n",
    "        # Densa\n",
    "        layers.Dense(512, kernel_initializer=\"he_normal\", kernel_regularizer=l2),\n",
    "        layers.BatchNormalization(),\n",
    "        layers.Activation('relu'),\n",
    "        layers.Dropout(0.30),\n",
    "\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "\n",
    "def train_model(model, train_dir, validation_dir, image_size=(128,128), batch_size=16, epochs=10, save_dir=\"./histories\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    os.environ[\"TF_DETERMINISTIC_OPS\"] = \"0\"\n",
    "    AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "    print(\"Carregando datasets...\")\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        train_dir,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    val_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        validation_dir,\n",
    "        image_size=image_size,\n",
    "        batch_size=batch_size,\n",
    "        label_mode='categorical',\n",
    "        shuffle=False\n",
    "    )\n",
    "\n",
    "    # otimizaÃ§Ã£o p/ GPU\n",
    "    train_dataset = train_dataset.cache().prefetch(AUTOTUNE)\n",
    "    val_dataset = val_dataset.cache().prefetch(AUTOTUNE)\n",
    "\n",
    "    # Callbacks\n",
    "    reduce_lr = ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=1,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        filepath=os.path.join(save_dir, 'best_model.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"Iniciando treino...\")\n",
    "    history = model.fit(\n",
    "        train_dataset,\n",
    "        epochs=epochs,\n",
    "        validation_data=val_dataset,\n",
    "        callbacks=[reduce_lr, early_stop, checkpoint]\n",
    "    )\n",
    "\n",
    "    # Salvar histÃ³rico\n",
    "    history_file = os.path.join(save_dir, 'history.csv')\n",
    "    pd.DataFrame(history.history).to_csv(history_file, index=False)\n",
    "    print(f\"HistÃ³rico salvo em: {history_file}\")\n",
    "    print(f\"Modelo salvo em: {os.path.join(save_dir, 'best_model.keras')}\")\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "# ConfiguraÃ§Ãµes da imagem\n",
    "image_size = (128, 128)\n",
    "batch_size = 64\n",
    "epochs = 40\n",
    "\n",
    "# Detectando automaticamente as classes pelas pastas do treino\n",
    "class_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes detectadas: {class_names}\")\n",
    "print(f\"NÃºmero de classes: {num_classes}\")\n",
    "\n",
    "# Criando o modelo\n",
    "model = build_model(image_size=image_size, num_classes=num_classes)\n",
    "\n",
    "# Treinando o modelo\n",
    "history = train_model(\n",
    "    model,\n",
    "    train_dir,\n",
    "    validation_dir,\n",
    "    image_size,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epochs,\n",
    "    save_dir=\"./histories\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AcurÃ¡cia de validaÃ§Ã£o mÃ¡xima: 0.4790 (47.90%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GeraÃ§Ã£o de VariaÃ§Ãµes de Imagens (Data Augmentation Offline)\n",
    "\n",
    "Este bloco de cÃ³digo implementa um procedimento de *data augmentation offline*, destinado a ampliar o conjunto de dados utilizado no treinamento de modelos de visÃ£o computacional. A tÃ©cnica consiste em gerar mÃºltiplas variaÃ§Ãµes artificiais de cada imagem original, com o objetivo de melhorar a capacidade de generalizaÃ§Ã£o do modelo e mitigar problemas de sobreajuste (*overfitting*).\n",
    "\n",
    "As variaÃ§Ãµes aplicadas incluem:\n",
    "\n",
    "- RotaÃ§Ãµes em Ã¢ngulos fixos e aleatÃ³rios;  \n",
    "- Espelhamento horizontal e vertical;  \n",
    "- AlteraÃ§Ãµes aleatÃ³rias de brilho;  \n",
    "- AlteraÃ§Ãµes aleatÃ³rias de contraste;  \n",
    "- InserÃ§Ã£o de ruÃ­do na imagem;  \n",
    "- Redimensionamento para padronizaÃ§Ã£o.\n",
    "\n",
    "As imagens geradas sÃ£o salvas em um diretÃ³rio especÃ­fico, mantendo-se a estrutura de categorias existente no diretÃ³rio de origem. Esse processo permite aumentar a diversidade do conjunto de treinamento e aproximar o modelo de situaÃ§Ãµes reais de variaÃ§Ã£o visual, contribuindo para um desempenho mais consistente durante a inferÃªncia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== GERANDO VARIAÃ‡Ã•ES ===\n",
      "\n",
      "ğŸ“‚ Categoria: pinenapple\n",
      "âœ” Total gerado em 'pinenapple': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: avocado\n",
      "âœ” Total gerado em 'avocado': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: cherry\n",
      "âœ” Total gerado em 'cherry': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: Apple\n",
      "âœ” Total gerado em 'Apple': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: strawberries\n",
      "âœ” Total gerado em 'strawberries': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: watermelon\n",
      "âœ” Total gerado em 'watermelon': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: kiwi\n",
      "âœ” Total gerado em 'kiwi': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: mango\n",
      "âœ” Total gerado em 'mango': 3465\n",
      "\n",
      "ğŸ“‚ Categoria: Banana\n",
      "âœ” Total gerado em 'Banana': 3450\n",
      "\n",
      "ğŸ“‚ Categoria: orange\n",
      "âœ” Total gerado em 'orange': 3450\n",
      "\n",
      "ğŸ‰ PRONTO! TODAS AS VARIAÃ‡Ã•ES FORAM GERADAS.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# FUNÃ‡ÃƒO: Criar variaÃ§Ãµes de uma Ãºnica imagem\n",
    "# --------------------------------------------------------------------\n",
    "def criar_variacoes(imagem_path, output_dir):\n",
    "    img = Image.open(imagem_path).convert('RGB')\n",
    "    W, H = img.size\n",
    "    variacoes = []\n",
    "\n",
    "    # RotaÃ§Ãµes\n",
    "    for angulo in [90, 180, 270, random.randint(10, 50)]:\n",
    "        variacoes.append(img.rotate(angulo).resize((W,H)))\n",
    "\n",
    "    # Espelhamentos\n",
    "    variacoes.append(img.transpose(Image.FLIP_LEFT_RIGHT))\n",
    "    variacoes.append(img.transpose(Image.FLIP_TOP_BOTTOM))\n",
    "\n",
    "    # Brilho\n",
    "    enhancer_b = ImageEnhance.Brightness(img)\n",
    "    for _ in range(3):\n",
    "        variacoes.append(enhancer_b.enhance(random.uniform(0.6, 1.4)))\n",
    "\n",
    "    # Contraste\n",
    "    enhancer_c = ImageEnhance.Contrast(img)\n",
    "    for _ in range(3):\n",
    "        variacoes.append(enhancer_c.enhance(random.uniform(0.6, 1.4)))\n",
    "\n",
    "    # RuÃ­do\n",
    "    arr = np.array(img)\n",
    "    for _ in range(3):\n",
    "        noise = np.random.randint(-20, 20, arr.shape, dtype='int16')\n",
    "        noisy = np.clip(arr.astype('int16') + noise, 0, 255).astype('uint8')\n",
    "        variacoes.append(Image.fromarray(noisy))\n",
    "\n",
    "    # Salvar\n",
    "    base = os.path.splitext(os.path.basename(imagem_path))[0]\n",
    "    for i, v in enumerate(variacoes):\n",
    "        out_path = os.path.join(output_dir, f\"{base}_var{i}.jpg\")\n",
    "        v.save(out_path, quality=95)\n",
    "        # print(f\"[OK] {out_path}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# FUNÃ‡ÃƒO: Criar variaÃ§Ãµes para todas as pastas\n",
    "# --------------------------------------------------------------------\n",
    "def gerar_variacoes_em_diretorio(diretorio_origem, diretorio_saida):\n",
    "    os.makedirs(diretorio_saida, exist_ok=True)\n",
    "\n",
    "    for categoria in os.listdir(diretorio_origem):\n",
    "        origem_cat = os.path.join(diretorio_origem, categoria)\n",
    "\n",
    "        if os.path.isdir(origem_cat):\n",
    "            saida_cat = os.path.join(diretorio_saida, categoria)\n",
    "            os.makedirs(saida_cat, exist_ok=True)\n",
    "\n",
    "            print(f\"\\nğŸ“‚ Categoria: {categoria}\")\n",
    "            for arquivo in os.listdir(origem_cat):\n",
    "                caminho = os.path.join(origem_cat, arquivo)\n",
    "\n",
    "                if arquivo.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    criar_variacoes(caminho, saida_cat)\n",
    "\n",
    "            total = len(os.listdir(saida_cat))\n",
    "            print(f\"âœ” Total gerado em '{categoria}': {total}\")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# EXECUTAR AQUI\n",
    "# --------------------------------------------------------------------\n",
    "train_dir = \"/home/wallingson12/Github/Data science and ML/Classificador de frutas/train\"\n",
    "output_dir = \"/home/wallingson12/Github/Data science and ML/Classificador de frutas/train_variacoes\"\n",
    "\n",
    "print(\"=== GERANDO VARIAÃ‡Ã•ES ===\")\n",
    "gerar_variacoes_em_diretorio(train_dir, output_dir)\n",
    "print(\"\\nğŸ‰ PRONTO! TODAS AS VARIAÃ‡Ã•ES FORAM GERADAS.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinar novamente"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como precisei aumentar a quantidade de imagens, o treino ficou mais pesado.\n",
    "Este bloco ativa **memory growth** na GPU, permitindo que o TensorFlow utilize a memÃ³ria de forma gradual, evitando erros de falta de VRAM.\n",
    "O **MLflow** registra parÃ¢metros, mÃ©tricas e o modelo final, permitindo acompanhar e comparar experimentos facilmente.\n",
    "Os datasets sÃ£o criados e otimizados com `tf.data`, garantindo eficiÃªncia mesmo com um nÃºmero maior de imagens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[XLA] Desativando completamente...\n",
      "[XLA] OFF âœ”ï¸\n",
      "[GPU] memory_growth ativado â†’ PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n",
      "[GPU] ConfiguraÃ§Ãµes aplicadas.\n",
      "\n",
      "Classes detectadas: ['Apple', 'Banana', 'avocado', 'cherry', 'kiwi', 'mango', 'orange', 'pinenapple', 'strawberries', 'watermelon']\n",
      "NÃºmero de classes: 10\n",
      "Found 34515 files belonging to 10 classes.\n",
      "Using 27612 files for training.\n",
      "Found 34515 files belonging to 10 classes.\n",
      "Using 6903 files for validation.\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wallingson12/.local/lib/python3.10/site-packages/mlflow/tracking/_tracking_service/utils.py:140: FutureWarning: Filesystem tracking backend (e.g., './mlruns') is deprecated. Please switch to a database backend (e.g., 'sqlite:///mlflow.db'). For feedback, see: https://github.com/mlflow/mlflow/issues/18534\n",
      "  return FileStore(store_uri, store_uri)\n",
      "E0000 00:00:1764440230.055967    4624 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/sequential_1_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m250s\u001b[0m 71ms/step - accuracy: 0.4721 - loss: 1.7962 - val_accuracy: 0.6452 - val_loss: 1.3785\n",
      "Epoch 2/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m247s\u001b[0m 72ms/step - accuracy: 0.5951 - loss: 1.4446 - val_accuracy: 0.7463 - val_loss: 1.0028\n",
      "Epoch 3/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 71ms/step - accuracy: 0.6597 - loss: 1.2543 - val_accuracy: 0.8233 - val_loss: 0.8632\n",
      "Epoch 4/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m255s\u001b[0m 74ms/step - accuracy: 0.7110 - loss: 1.1145 - val_accuracy: 0.9410 - val_loss: 0.4875\n",
      "Epoch 5/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 91ms/step - accuracy: 0.7425 - loss: 1.0189 - val_accuracy: 0.9015 - val_loss: 0.5964\n",
      "Epoch 6/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m313s\u001b[0m 91ms/step - accuracy: 0.7698 - loss: 0.9393 - val_accuracy: 0.8770 - val_loss: 0.6445\n",
      "Epoch 7/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m324s\u001b[0m 94ms/step - accuracy: 0.7906 - loss: 0.8778 - val_accuracy: 0.8990 - val_loss: 0.5972\n",
      "Epoch 8/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m318s\u001b[0m 92ms/step - accuracy: 0.8081 - loss: 0.8150 - val_accuracy: 0.8954 - val_loss: 0.5971\n",
      "Epoch 9/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m312s\u001b[0m 90ms/step - accuracy: 0.8240 - loss: 0.7635 - val_accuracy: 0.9405 - val_loss: 0.4650\n",
      "Epoch 10/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m305s\u001b[0m 88ms/step - accuracy: 0.8360 - loss: 0.7261 - val_accuracy: 0.7882 - val_loss: 1.0245\n",
      "Epoch 11/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m306s\u001b[0m 88ms/step - accuracy: 0.8513 - loss: 0.6789 - val_accuracy: 0.8524 - val_loss: 0.6856\n",
      "Epoch 12/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m309s\u001b[0m 90ms/step - accuracy: 0.8653 - loss: 0.6412 - val_accuracy: 0.9448 - val_loss: 0.4420\n",
      "Epoch 13/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m323s\u001b[0m 94ms/step - accuracy: 0.8733 - loss: 0.6162 - val_accuracy: 0.8517 - val_loss: 0.7412\n",
      "Epoch 14/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m286s\u001b[0m 83ms/step - accuracy: 0.8815 - loss: 0.5874 - val_accuracy: 0.9053 - val_loss: 0.5403\n",
      "Epoch 15/15\n",
      "\u001b[1m3452/3452\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m245s\u001b[0m 71ms/step - accuracy: 0.8914 - loss: 0.5592 - val_accuracy: 0.9258 - val_loss: 0.4724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/11/29 16:29:41 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during tensorflow autologging: Changing param values is not allowed. Param with key='batch_size' was already logged with value='8' for run ID='20245f9d94e040eb819b34b1c3d4ea16'. Attempted logging new value 'None'.\n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Modelo salvo como modelo_frutas.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ğŸ”¥ DESATIVAR XLA (correto e realmente funcional)\n",
    "# -----------------------------------------------------------\n",
    "print(\"[XLA] Desativando completamente...\")\n",
    "os.environ[\"TF_XLA_FLAGS\"] = \"--tf_xla_enable_xla_devices=false\"\n",
    "os.environ[\"XLA_FLAGS\"] = \"--xla_gpu_cuda_data_dir=''\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "print(\"[XLA] OFF âœ”ï¸\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import mlflow\n",
    "import mlflow.tensorflow\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# ğŸ”¥ GPU SAFE MODE (evita OOM)\n",
    "# -----------------------------------------------------------\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    for gpu in gpus:\n",
    "        try:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"[GPU] memory_growth ativado â†’ {gpu}\")\n",
    "        except:\n",
    "            pass\n",
    "print(\"[GPU] ConfiguraÃ§Ãµes aplicadas.\\n\")\n",
    "\n",
    "# ---- MLflow autolog ----\n",
    "mlflow.tensorflow.autolog()\n",
    "\n",
    "train_dir = \"/home/wallingson12/Github/Data science and ML/Classificador de frutas/train_variacoes\"\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "#  DETECTAR CLASSES\n",
    "# -----------------------------------------------------------\n",
    "image_size = (128, 128)\n",
    "\n",
    "# ğŸ”¥ batch menor ajuda a evitar OOM\n",
    "batch_size = 8\n",
    "\n",
    "epochs = 15\n",
    "validation_split = 0.2\n",
    "\n",
    "class_names = sorted([d for d in os.listdir(train_dir) if os.path.isdir(os.path.join(train_dir, d))])\n",
    "num_classes = len(class_names)\n",
    "\n",
    "print(f\"Classes detectadas: {class_names}\")\n",
    "print(f\"NÃºmero de classes: {num_classes}\")\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# DATASETS\n",
    "# -----------------------------------------------------------\n",
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True,\n",
    "    validation_split=validation_split,\n",
    "    subset='training',\n",
    "    seed=42\n",
    ").prefetch(1)\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    train_dir,\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    label_mode='categorical',\n",
    "    shuffle=False,\n",
    "    validation_split=validation_split,\n",
    "    subset='validation',\n",
    "    seed=42\n",
    ").prefetch(1)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# MODELO\n",
    "# -----------------------------------------------------------\n",
    "model = build_model(image_size=image_size, num_classes=num_classes)\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# TREINO + MLflow\n",
    "# -----------------------------------------------------------\n",
    "with mlflow.start_run():\n",
    "\n",
    "    mlflow.log_param(\"image_size\", image_size)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"epochs\", epochs)\n",
    "    mlflow.log_param(\"num_classes\", num_classes)\n",
    "    mlflow.log_param(\"validation_split\", validation_split)\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=epochs\n",
    "    )\n",
    "\n",
    "    for epoch in range(len(history.history[\"loss\"])):\n",
    "        mlflow.log_metric(\"train_loss\", history.history[\"loss\"][epoch], epoch)\n",
    "        mlflow.log_metric(\"train_accuracy\", history.history[\"accuracy\"][epoch], epoch)\n",
    "        mlflow.log_metric(\"val_loss\", history.history[\"val_loss\"][epoch], epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", history.history[\"val_accuracy\"][epoch], epoch)\n",
    "\n",
    "    model.save(\"modelo_frutas.h5\")\n",
    "    mlflow.log_artifact(\"modelo_frutas.h5\")\n",
    "\n",
    "print(\"\\nâœ… Modelo salvo como modelo_frutas.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AcurÃ¡cia de validaÃ§Ã£o mÃ¡xima: 0.4790 (93%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VisualizaÃ§Ã£o do Treinamento â€” AcurÃ¡cia e Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Plota acurÃ¡cia e loss do treinamento a partir do objeto History do Keras.\n",
    "    \"\"\"\n",
    "    # AcurÃ¡cia\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(history.history['accuracy'], marker='o')\n",
    "    plt.plot(history.history['val_accuracy'], marker='o')\n",
    "    plt.title('Treino x ValidaÃ§Ã£o - AcurÃ¡cia')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('AcurÃ¡cia')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['Treino', 'ValidaÃ§Ã£o'])\n",
    "    plt.show()\n",
    "\n",
    "    # Loss\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(history.history['loss'], marker='o')\n",
    "    plt.plot(history.history['val_loss'], marker='o')\n",
    "    plt.title('Treino x ValidaÃ§Ã£o - Loss')\n",
    "    plt.xlabel('Ã‰poca')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.legend(['Treino', 'ValidaÃ§Ã£o'])\n",
    "    plt.show()\n",
    "\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa lista mostra **apenas a precisÃ£o de cada classe**, ou seja, a proporÃ§Ã£o de previsÃµes corretas entre todas as previsÃµes feitas para aquela classe especÃ­fica.\n",
    "\n",
    "Alguns pontos importantes:\n",
    "\n",
    "- Classes como **kiwi, watermelon e pinenapple** tÃªm precisÃ£o muito alta (>0.85), o que indica que o modelo raramente erra quando prediz essas frutas.\n",
    "- Classes como **avocado e banana** tÃªm precisÃ£o muito baixa (<0.05), mostrando que o modelo frequentemente confunde essas frutas com outras.\n",
    "- O modelo parece **tender a prever certas classes com mais confianÃ§a** e confundir outras, possivelmente por ter **menos exemplos de treino ou mais similaridade visual** entre frutas.\n",
    "- Isso ajuda a identificar **onde melhorar o dataset ou ajustar o modelo**, por exemplo: aumentar imagens de avocado/banana ou aplicar tÃ©cnicas de balanceamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Matriz de confusÃ£o\n",
    "cm = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# PrecisÃ£o por classe = diagonal / soma da coluna\n",
    "precision_per_class = np.diag(cm) / cm.sum(axis=0)\n",
    "\n",
    "# Print formatado como tabela\n",
    "print(\"PrecisÃ£o por Classe:\\n\")\n",
    "for cls, prec in zip(class_names, precision_per_class):\n",
    "    print(f\"{cls:12s} : {prec:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOZWucd0eVVwgdp9rwVxAdi",
   "mount_file_id": "10gq2KS9SbGrOV8Fq5Ti3fak_eE1Mk2sG",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
